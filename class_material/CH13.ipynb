{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eed8e99",
   "metadata": {},
   "source": [
    "# 建立VGG16網路模型來辨識貓狗數據集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b242a0c3",
   "metadata": {},
   "source": [
    "1. 載入資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b2f46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# 數據讀取\n",
    "TrainDataGenerator = ImageDataGenerator()\n",
    "traindata = TrainDataGenerator.flow_from_directory(\n",
    "                    directory=\"Cats&Dogs/train\",target_size=(224,224))\n",
    "TestDataGenerator = ImageDataGenerator()\n",
    "testdata = TestDataGenerator.flow_from_directory(\n",
    "                    directory=\"Cats&Dogs/test\", target_size=(224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaec6d9",
   "metadata": {},
   "source": [
    "2. 模型建立"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744488f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
    "model = Sequential([\n",
    "    #　第一組 :兩個 3*3*64 卷積核 + 一個最大池化層\n",
    "    Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding=\"same\",\n",
    "           activation=\"relu\"),\n",
    "    Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"),\n",
    "    MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
    "    #　第二組 : 兩個3*3*128卷積核 + 一個最大池化層\n",
    "    Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
    "    #　第三組 : 三個3*3*56卷積核 + 一個最大池化層\n",
    "    Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
    "    #　第四組 : 三個3*3*512卷積核 + 一個最大池化層\n",
    "    Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
    "    #　第五組 : 三個3*3*512卷積核 + 一個最大池化層\n",
    "    Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "    MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
    "    # 三個全連接層Dense，最後一層用於預測分類。\n",
    "    Flatten(),\n",
    "    Dense(units=4096,activation=\"relu\"),\n",
    "    Dense(units=4096,activation=\"relu\"),\n",
    "    Dense(units=2, activation=\"softmax\")\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce826bad",
   "metadata": {},
   "source": [
    "3. 編譯模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867f4fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 編譯模型, 定義模型優化器， 使用分類交叉熵損失\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras\n",
    "model.compile(optimizer=Adam(lr=0.00001),\n",
    "              loss = tensorflow.keras.losses.categorical_crossentropy,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6cf6c0",
   "metadata": {},
   "source": [
    "4. 設定模型儲存條件與提早停止訓練條件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fa69a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定監控方法與條件\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# 模型儲存名稱為 vgg16.h5, 監控的評估參數為 val_accuracy\n",
    "checkpoint = ModelCheckpoint(\"vgg16.h5\", monitor='val_accuracy', verbose=1,\n",
    "                          save_best_only=True,save_weights_only=False,\n",
    "                          mode='auto', save_freq=1)\n",
    "earlystop = EarlyStopping(monitor='val_accuracy', min_delta=0,\n",
    "                          patience=20, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9947ec",
   "metadata": {},
   "source": [
    "5. 開始訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b287dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練模型並呼叫回調函數\n",
    "history = model.fit_generator(steps_per_epoch=100,generator=traindata,\n",
    "                              validation_data= testdata,\n",
    "                              validation_steps=10,epochs=50,\n",
    "                              callbacks=[checkpoint,earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b258c75",
   "metadata": {},
   "source": [
    "6. 最後我們將程式執行到最後的結果擷取出，發現該模型的正確率約為96%。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf80a641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Accuracy\",\"Validation Accuracy\",\"loss\",\"Validation Loss\"])\n",
    "plt.show(block=True)\n",
    "\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3841be4a",
   "metadata": {},
   "source": [
    "# 載入模型與資料預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0798c1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# 載入影像後做一個尺度大小設定\n",
    "img = image.load_img(\"cat001.jpg\",target_size=(224,224))\n",
    "img = np.asarray(img)\n",
    "plt.imshow(img)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "plt.show(block=True)\n",
    "# 載入模型\n",
    "saved_model = load_model(\"vgg16.h5\")\n",
    "# 模型預測\n",
    "output = saved_model.predict(img)\n",
    "if output[0][0] > output[0][1]:\n",
    "    print(\"cat\")\n",
    "else:\n",
    "    print('dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a28fde",
   "metadata": {},
   "source": [
    "# Keras fit_generator 函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae53baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "EPOCHS = 100\n",
    "BS = 32\n",
    "\n",
    "aug = IMageDataGenerator(rotation_range=20,zoom_range=0.15,width_shift_range=0.2,\n",
    "                         height_shift_range=0.2, shear_range=0.15, horizontal_flip=True, fill_mode=\"nearest\")\n",
    "history = model.fit_generator(aug.flow(trainX, trainY, batch_size=BS), validation_data=(testX, testY),\n",
    "                              steps_per_epoch = len(trainX)//BS, epochs = EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb01a8f0",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13f22d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# 使用numpy生成100個隨機點\n",
    "np.random.seed(5)\n",
    "x_data = np.random.rand(100)\n",
    "noise = np.random.mormal(0,0.01,x_data.shape)\n",
    "y_data = x_data*0.1+0.2+noise   # 加入噪聲，便於線性回歸\n",
    "# 顯示隨即點\n",
    "plt.scatter(x_data,y_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd611219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立模型，並加入一個全連接層\n",
    "model = Sequential()\n",
    "model.add(Dense(units=1, input_dim=1))\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "def generator(x_data, y_data, size):\n",
    "    while True:\n",
    "        for i in range(size):\n",
    "            x = x_data[i*size:(i+1)*size-1]\n",
    "            y = y_data[i*size:(i+1)*size-1]\n",
    "            yield x,y\n",
    "\n",
    "# 以下使用了fit, fit_generator與train_on_batch\n",
    "# model.fit(x_data, y_data, epochs=1000, batch_size=10)\n",
    "# model.fit_generator(generator(x_data, y_data, 10), steps_per_epoch=10, epochs=1000, verbose=1)\n",
    "for step in range(10001):\n",
    "    cost = model.train_on_batch(x_data, y_data)\n",
    "    if step%5 == 0:\n",
    "        print('cost', cost)\n",
    "w,b = model.layers[0].get_weights()\n",
    "print('w:',w,'b:',b)\n",
    "\n",
    "# 輸入x，輸出預測值y\n",
    "y_pred = model.predict(x_data)\n",
    "# 顯示隨機點\n",
    "plt.scatter(x_data, y_data)\n",
    "# 顯示預測結果\n",
    "plt.plot(x_data, y_pred, 'r-', lw=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4012d7",
   "metadata": {},
   "source": [
    "# GoogLeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde64c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model, models, Sequential\n",
    "\n",
    "class Inception(layers.Layer):\n",
    "    def __init__(self, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj, **kwargs):\n",
    "        super(Inception, self).__init__(**kwargs)\n",
    "        # 定義四個分支\n",
    "        # 第一個分支為 1x1 的卷積\n",
    "        self.branch1 = layers.Conv2D(ch1x1, kernel_size=1, activation=\"relu\")\n",
    "        # 第二個分支為 1x1 的卷積 + 3x3的卷積 (輸出的大小 = 輸入的大小)\n",
    "        self.branch2 = Sequential([\n",
    "            layers.Conv2D(ch3x3red, kernel_size=1, activation=\"relu\"),\n",
    "            layers.Conv2D(ch3x3, kernel_size=3, padding=\"SAME\", activation=\"relu\")])     \n",
    "        # 第三個分支為 1x1 的卷積 + 5x5的卷積 (輸出的大小 = 輸入的大小)\n",
    "        self.branch3 = Sequential([\n",
    "            layers.Conv2D(ch5x5red, kernel_size=1, activation=\"relu\"),\n",
    "            layers.Conv2D(ch5x5, kernel_size=5, padding=\"SAME\", activation=\"relu\")])      \n",
    "        # 第四個分支為 3x3 的最大池化 + 1x1的卷積  (輸出的大小 = 輸入的大小)\n",
    "        self.branch4 = Sequential([\n",
    "            layers.MaxPool2D(pool_size=3, strides=1, padding=\"SAME\"),  \n",
    "            layers.Conv2D(pool_proj, kernel_size=1, activation=\"relu\")])                  \n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        branch1 = self.branch1(inputs)   # 輸入的特徵圖分別進入分支 1 至 4\n",
    "        branch2 = self.branch2(inputs)\n",
    "        branch3 = self.branch3(inputs)\n",
    "        branch4 = self.branch4(inputs)\n",
    "        outputs = layers.concatenate([branch1, branch2, branch3, branch4])\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf88d5e0",
   "metadata": {},
   "source": [
    "輔助分類器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324b9016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 輔助分類器\n",
    "class InceptionAux(layers.Layer):\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        super(InceptionAux, self).__init__(**kwargs)\n",
    "        # 第一層 :平均池化層\n",
    "        self.averagePool = layers.AvgPool2D(pool_size=5, strides=3)\n",
    "        # 第二層 :1x1卷積層\n",
    "        self.conv = layers.Conv2D(128, kernel_size=1, activation=\"relu\")\n",
    "        # 第三層 :全連接層\n",
    "        self.fc1 = layers.Dense(1024, activation=\"relu\")\n",
    "        # 第四層 :全連接層, 節點個數是非類數目\n",
    "        self.fc2 = layers.Dense(num_classes)\n",
    "        # 第五層 :softmax\n",
    "        self.softmax = layers.Softmax()\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = self.averagePool(inputs)\n",
    "        x = self.conv(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dropout(rate=0.5)(x)\n",
    "        x = self.fc1(x)\n",
    "        x = layers.Dropout(rate=0.5)(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209cb305",
   "metadata": {},
   "source": [
    "第一、二模塊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bf1d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GoogLeNet(im_height=224, im_width=224, class_num=1000, aux_logits=False):\n",
    "    # tensorflow中的tensor通道排序是NHWC\n",
    "    input_image = layers.Input(shape=(im_height, im_width, 3), dtype=\"float32\")\n",
    "    # (1)第一模塊 :\n",
    "    # (None, 224, 224, 3)\n",
    "    x = layers.Conv2D(64, kernel_size=7, strides=2, padding=\"SAME\", activation=\"relu\", name=\"conv2d_1\")(input_image)\n",
    "    # (None, 112, 112, 64)\n",
    "    x = layers.MaxPool2D(pool_size=3, strides=2, padding=\"SAME\", name=\"maxpool_1\")(x)\n",
    "    # (2)第二模塊 :\n",
    "    # (None, 56, 56, 64)\n",
    "    x = layers.Conv2D(64, kernel_size=1, activation=\"relu\", name=\"conv2d_2\")(x)\n",
    "    # (None, 56, 56, 64)\n",
    "    x = layers.Conv2D(192, kernel_size=3, padding=\"SAME\", activation=\"relu\", name=\"conv2d_3\")(x)\n",
    "    # (None, 56, 56, 192)\n",
    "    x = layers.MaxPool2D(pool_size=3, strides=2, padding=\"SAME\", name=\"maxpool_2\")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8077cf",
   "metadata": {},
   "source": [
    "第三模塊(Inception 3a、3b層)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e6e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # (None, 28, 28, 192)\n",
    "    x = Inception(64, 96, 128, 16, 32, 32, name=\"inception_3a\")(x)\n",
    "    # (None, 28, 28, 256)\n",
    "    x = Inception(128, 128, 192, 32, 96, 64, name=\"inception_3b\")(x)\n",
    "    # (None, 28, 28, 480)\n",
    "    x = layers.MaxPool2D(pool_size=3, strides=2, padding=\"SAME\", name=\"maxpool_3\")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f9168f",
   "metadata": {},
   "source": [
    "第四模塊(Inception 4a、4b、4c、4d、4e層)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d840cbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # (None, 14, 14, 480)\n",
    "    x = Inception(192, 96, 208, 16, 48, 64, name=\"inception_4a\")(x)\n",
    "    if aux_logits:\n",
    "        aux1 = InceptionAux(class_num, name=\"aux_1\")(x)\n",
    "\n",
    "    # (None, 14, 14, 512)\n",
    "    x = Inception(160, 112, 224, 24, 64, 64, name=\"inception_4b\")(x)\n",
    "    # (None, 14, 14, 512)\n",
    "    x = Inception(128, 128, 256, 24, 64, 64, name=\"inception_4c\")(x)\n",
    "    # (None, 14, 14, 512)\n",
    "    x = Inception(112, 144, 288, 32, 64, 64, name=\"inception_4d\")(x)\n",
    "    if aux_logits:\n",
    "        aux2 = InceptionAux(class_num, name=\"aux_2\")(x)\n",
    "\n",
    "    # (None, 14, 14, 528)\n",
    "    x = Inception(256, 160, 320, 32, 128, 128, name=\"inception_4e\")(x)\n",
    "    # (None, 14, 14, 532)\n",
    "    x = layers.MaxPool2D(pool_size=3, strides=2, padding=\"SAME\", name=\"maxpool_4\")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f074cbb3",
   "metadata": {},
   "source": [
    "第五模塊與最後輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a659ea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # (None, 7, 7, 832)\n",
    "    x = Inception(256, 160, 320, 32, 128, 128, name=\"inception_5a\")(x)\n",
    "    # (None, 7, 7, 832)\n",
    "    x = Inception(384, 192, 384, 48, 128, 128, name=\"inception_5b\")(x)\n",
    "    # (None, 7, 7, 1024)\n",
    "    x = layers.AvgPool2D(pool_size=7, strides=1, name=\"avgpool_1\")(x)\n",
    "    \n",
    "    # (None, 1, 1, 1024)\n",
    "    x = layers.Flatten(name=\"output_flatten\")(x)\n",
    "    # (None, 1024)\n",
    "    x = layers.Dropout(rate=0.4, name=\"output_dropout\")(x)\n",
    "    x = layers.Dense(class_num, name=\"output_dense\")(x)\n",
    "    # (None, class_num)\n",
    "    aux3 = layers.Softmax(name=\"aux_3\")(x)\n",
    "\n",
    "    if aux_logits:\n",
    "        model = models.Model(inputs=input_image, outputs=[aux1, aux2, aux3])\n",
    "    else:\n",
    "        model = models.Model(inputs=input_image, outputs=aux3)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5b59b1",
   "metadata": {},
   "source": [
    "產生模型實體與定義模型輸入大小與輸出類別大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8cec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_height = 224\n",
    "im_width = 224\n",
    "batch_size = 32\n",
    "model = GoogLeNet(im_height=im_height, im_width=im_width, class_num=10, aux_logits=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692b62e3",
   "metadata": {},
   "source": [
    "定義模型使用的優化函數與損失函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4399a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# 使用 keras 所提供的優化函數與損失函數\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0003)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5714b99",
   "metadata": {},
   "source": [
    "定義訓練階段函數與驗證階段函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c557c842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義訓練階段函數\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        aux1, aux2, output = model(images, training=True)\n",
    "        loss1 = loss_object(labels, aux1)\n",
    "        loss2 = loss_object(labels, aux2)\n",
    "        loss3 = loss_object(labels, output)\n",
    "        loss = loss1 * 0.3 + loss2 * 0.3 + loss3\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # 計算訓練的效能衡量指標\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, output)\n",
    "# 定義驗證階段函數   \n",
    "@tf.function\n",
    "def val_step(images, labels):\n",
    "    _, _, output = model(images, training=False)\n",
    "    loss = loss_object(labels, output)\n",
    "    val_loss(loss)\n",
    "    val_accuracy(labels, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40a66c9",
   "metadata": {},
   "source": [
    "載入資料集並進行前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b0f076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "import tensorflow as tf\n",
    "# 載入 cifar10 資料集 50000張訓練資料 , 10000張測試資料, 每張大小為 32x32,3通道\n",
    "(train_Data, train_Label), (test_Data, test_Label) = cifar10.load_data()\n",
    "\n",
    "# 資料切割, 訓練資料的前面 5000 筆當作是驗證集, 剩下的為測試集\n",
    "validation_data, validation_label = train_Data[:5000],train_Label[:5000]\n",
    "train_Data,train_Label= train_Data[5000:],train_Label[5000:]\n",
    "\n",
    "batchs = 32\n",
    "resize = 224\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_Data,train_Label))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_Data, test_Label))\n",
    "validation_ds = tf.data.Dataset.from_tensor_slices((validation_data,\n",
    "                                                    validation_label))\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    image = tf.image.resize(image,(resize,resize))\n",
    "    return image,label\n",
    "\n",
    "batch_size = 360\n",
    "train_ds = train_ds.map(preprocess).shuffle(1000).batch(batch_size=batch_size)\n",
    "# 驗證集與測試集資料不用打散\n",
    "validation_ds = validation_ds.map(preprocess).batch(batch_size=batch_size)\n",
    "test_ds = test_ds.map(preprocess).batch(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b353fd25",
   "metadata": {},
   "source": [
    "開始進行訓練，並在每一個 epoch 結束後進行驗證集驗證"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e789fe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    train_loss.reset_states()      # clear history info\n",
    "    train_accuracy.reset_states()  # clear history info\n",
    "    val_loss.reset_states()        # clear history info\n",
    "    val_accuracy.reset_states()    # clear history info\n",
    "    print(\"epoch = \",epoch + 1)\n",
    "    for step,(batch_data,batch_label) in enumerate(train_ds):\n",
    "        train_step(batch_data,batch_label)\n",
    "        print(\"train_accuracy = \",train_accuracy.result())\n",
    "        print(\"train_loss = \",train_loss.result())\n",
    "    \n",
    "    for step,(batch_data,batch_label) in enumerate(test_ds):\n",
    "        val_step(batch_data, batch_label)\n",
    "    # 每一次 epoch 印出驗證集的正確率與損失率\n",
    "    print(\"val_accuracy = \",val_accuracy.result())\n",
    "    print(\"val_loss = \",val_loss.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1a665b",
   "metadata": {},
   "source": [
    "# BasicBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8613b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,Sequential\n",
    "\n",
    "class BasicBlock(layers.Layer):\n",
    "    # 定義殘差模塊類別\n",
    "    def __init__(self, filter_num, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        # f(x)包含了 2 個普通卷積層，創建卷積層 1 =>(3x3),64\n",
    "        self.conv1 = layers.Conv2D(filter_num, (3, 3),\n",
    "                                   strides=stride, padding = 'same')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.relu = layers.Activation('relu')\n",
    "        # 創建卷積層 2  =>(3x3),64\n",
    "        self.conv2 = layers.Conv2D(filter_num, (3, 3), strides=1,\n",
    "                                   padding='same')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        if stride != 1:  # 插入 identity 層 (stride!=1需要下採樣)\n",
    "            self.downsample = Sequential()\n",
    "            self.downsample.add(layers.Conv2D(filter_num, (1, 1),\n",
    "                                              strides=stride))\n",
    "        else:  # 否則，直接連接\n",
    "            self.downsample = lambda x: x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d790a7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def call(self, inputs, training=None):\n",
    "        # 前向計算\n",
    "        out = self.conv1(inputs) # 通過第一個卷積層\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out) # 通過第二個卷積層\n",
    "        out = self.bn2(out)\n",
    "        # inputs 通過 identity()轉換\n",
    "        identity = self.downsample(inputs)\n",
    "        # f(x)+x 運算\n",
    "        output = layers.add([out, identity])\n",
    "        # 再通過relu激勵函數計算並傳回\n",
    "        output = tf.nn.relu(output)\n",
    "        return output     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3306ac7a",
   "metadata": {},
   "source": [
    "利用堆疊方式來擴大ResBlock模塊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51bc401",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 製作多個殘差模塊的堆疊\n",
    "    def build_resblock(self, filter_num, blocks, stride=1):\n",
    "\n",
    "        Resblock = Sequential()\n",
    "        # 堆疊的第一個 BasicBlock 步長不會是 1, 所以進行下採樣\n",
    "        Resblock.add(BasicBlock(filter_num, stride))\n",
    "\n",
    "        for _ in range(1, blocks):  # 其他 BasicBlock 步長都為1\n",
    "            # 這裏stride設置爲1，只會在第一個Basic Block做一個下采樣。\n",
    "            Resblock.add(BasicBlock(filter_num, stride=1))\n",
    "        return Resblock            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd557b73",
   "metadata": {},
   "source": [
    "利用build_resblock完成Resnet18網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbce681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "# 設定 ResBlock 模塊數目內部通道數。\n",
    "class ResNet(keras.Model):\n",
    "    # 第一個參數 layer_dims：[2, 2, 2, 2] 共 4個 Res Block，\n",
    "    # 每個包含2個Basic Block\n",
    "    # 第二個參數 num_classes：設定全連接輸出個數，這邊是指輸出有多少類。\n",
    "    def __init__(self, layer_dims, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        # 預處理層；可以藉由此層來設定一開始的通道數與欲輸入的特徵圖大小\n",
    "        self.Prelayer = Sequential([\n",
    "            layers.Conv2D(64, (3, 3), strides=(1, 1)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Activation('relu'),\n",
    "            layers.MaxPool2D(pool_size=(2, 2), strides=(1, 1),\n",
    "                             padding='same')\n",
    "        ])\n",
    "\t\t\t\t# 創建4個Res Block\n",
    "        self.layer1 = self.build_resblock(64, layer_dims[0])\n",
    "        self.layer2 = self.build_resblock(128, layer_dims[1], stride=2)\n",
    "        self.layer3 = self.build_resblock(256, layer_dims[2], stride=2)\n",
    "        self.layer4 = self.build_resblock(512, layer_dims[3], stride=2)\n",
    "        # 通過 Pooling 層將寬與高降低為1x1\n",
    "        self.avgpool = layers.GlobalAveragePooling2D()\n",
    "        # 最後連接一個全連接層分類\n",
    "        self.fc = layers.Dense(num_classes,activation = 'softmax')\n",
    "\t\t\n",
    "\t\t# 完成Resnet前向計算流程\n",
    "    def call(self,inputs, training=None, **kwargs):\n",
    "        # 完成前向運算過程\n",
    "        x = self.Prelayer(inputs)\n",
    "        # 前項計算通過四個 resblock 模塊\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        # shape爲 [batchsize, channel]\n",
    "        x = self.avgpool(x)\n",
    "        # [b, 10]\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a045cc3",
   "metadata": {},
   "source": [
    "創建ResNet18 網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc15469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet18():\n",
    "    # 通過調整內部模塊個數可以完成不同的 resnet 網路\n",
    "    return ResNet([2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8123b100",
   "metadata": {},
   "source": [
    "# ResNet18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4f6b40",
   "metadata": {},
   "source": [
    "加載CIFAR10數據集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cf9f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from CH12_BasicBlock import resnet18\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "# 載入 cifar10 資料集 50000張訓練資料 , 10000張測試資料, 每張大小為 32x32,3通道\n",
    "(train_Data, train_Label), (test_Data, test_Label) = cifar10.load_data()\n",
    "\n",
    "# 將多餘的維度刪除\n",
    "train_Label = tf.squeeze(train_Label, axis=1)\n",
    "test_Label = tf.squeeze(test_Label, axis=1)\n",
    "# 新增驗證集, 將訓練集資料的前 5000比當作驗證集\n",
    "validation_data, validation_label = train_Data[:5000],train_Label[:5000]\n",
    "Newtrain_Data,Newtrain_Label= train_Data[5000:],train_Label[5000:]\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((Newtrain_Data, Newtrain_Label))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_Data, test_Label))\n",
    "validation_ds = tf.data.Dataset.from_tensor_slices((validation_data,\n",
    "                                                    validation_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1090b538",
   "metadata": {},
   "source": [
    "資料前處理與訓練資料打散"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90118df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, label):\n",
    "    image = tf.cast(image, dtype=tf.float32) / 255.\n",
    "    label = tf.cast(label, dtype=tf.int32)\n",
    "    return image,label\n",
    "\n",
    "batch_size = 256\n",
    "epoch = 30\n",
    "train_ds = train_ds.map(preprocess).shuffle(500).batch(batch_size=batch_size)\n",
    "# 驗證集與測試集資料不用打散\n",
    "test_ds = test_ds.map(preprocess).batch(batch_size=batch_size)\n",
    "validation_ds = validation_ds.map(preprocess).batch(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3cbbbd",
   "metadata": {},
   "source": [
    "網路編譯與訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc300e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18()\n",
    "model.build(input_shape=(None, 32, 32, 3))\n",
    "\n",
    "model.compile(optimizer= 'adam',loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "History = model.fit(train_ds,epochs=epoch,validation_data=validation_ds,validation_freq=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
