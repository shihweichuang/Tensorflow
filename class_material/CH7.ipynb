{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1a1b876",
   "metadata": {},
   "source": [
    "# 梯度下降法(gradient descent，GD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c47816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 設定超參數(Hyperparameters)值\n",
    "x_init = -10   # 起始權重\n",
    "epochs = 10    # 執行週期數，跑10次\n",
    "lr = 0.3       # 學習率\n",
    "\n",
    "# 定義損失函數 y = x^2 - 10x +1\n",
    "def Loss(x):\n",
    "    y = x ** 2 - 10*x + 1\n",
    "    return y\n",
    "\n",
    "# 定義梯度\n",
    "def dLoss(x_value):\n",
    "    # 宣告Tensorflow變數(Variable)\n",
    "    x = tf.Variable(x_value, dtype=tf.float32)\n",
    "    with tf.GradientTape() as g:   # 自動微分\n",
    "        y = Loss(x)\n",
    "        dy_dx = g.gradient(y, x)       # 取得梯度(y對x微分，求出斜率=梯度)\n",
    "    return dy_dx.numpy()           # 轉成Numpy array(陣列形式)\n",
    "\n",
    "# 定義梯度下降法\n",
    "def GD(x_init, df, epochs, lr):\n",
    "    xs = np.zeros(epochs+1)\n",
    "    x = x_init\n",
    "    xs[0] = x\n",
    "    for i in range(epochs):\n",
    "        dx = df(x)\n",
    "        # 梯度下降法公式=>更新x_new = x - learning_rate * gradient\n",
    "        x += -dx * lr \n",
    "        xs[i+1] = x\n",
    "    return xs\n",
    "\n",
    "# 傳入dLoss\n",
    "w = GD(x_init, dLoss, epochs, lr=lr)\n",
    "print(np.around(w, 2))\n",
    "\n",
    "t = np.arange(-10.0, 20.0, 0.001)\n",
    "plt.plot(t, Loss(t), c='b')\n",
    "plt.plot(w, Loss(w), c='r', marker = 'o', markersize = 5)\n",
    "\n",
    "# 設定中文字型\n",
    "plt.rcParams['font.sas-serif'] = ['Microsoft JhengHei'] # 正黑體\n",
    "plt.rcParams['axes.unicode_minus'] = False              # 矯正負號\n",
    "plt.title('梯度下降法', fontsize=18)\n",
    "plt.xlabel('X 參數值', fontsize=18)\n",
    "plt.ylabel('損失函數值', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d9008e",
   "metadata": {},
   "source": [
    "# 隨機梯度下降法（SGD，Stochastic Gradient Descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f80cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 初始值\n",
    "x = tf.Variable(-8.00000)\n",
    "y = tf.Variable(5.00000)\n",
    "\n",
    "# 定義目標函數\n",
    "def ObjFun():\n",
    "    output = (0.5)*(x**2)+2.5*(y**2)\n",
    "    return output\n",
    "\n",
    "# 繪圖 # 要繪圖的函數，與目標函數相同\n",
    "def Draw_fun(x,y):    \n",
    "    z = (0.5)*(x**2)+2.5*(y**2)\n",
    "    return z\n",
    "\n",
    "# 選用一優化函式，作為迭代方法\n",
    "opt = tf.keras.optimizers.SGD(learning_rate = 0.3)\n",
    "\n",
    "epochs = 10        # 疊代次數\n",
    "xValueArr = [-8]   # x的初始值\n",
    "yValueArr = [5]    # y的初始值\n",
    "for epoch in range(epochs):\n",
    "    opt.minimize(ObjFun, var_list=[x,y])   # 帶入函數，xy之最小值為何\n",
    "    xValueArr.append(x.numpy())\n",
    "    yValueArr.append(y.numpy())\n",
    "\n",
    "x = np.arange(-10.0, 10.0, 0.01)\n",
    "y = np.arange(-10.0, 10.0, 0.01)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = Draw_fun(X,Y)\n",
    "plt.figure(figsize = (10,5))\n",
    "CS = plt.contour(X,Y,Z, colors = 'gray')\n",
    "plt.title(\"Adamax Optimizer')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.plot(xValueArr, yValueArr, c='r')\n",
    "for xt, yt in zip(xValueArr,yValueArr):\n",
    "          plt.scatter(xt, yt, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dbb36c",
   "metadata": {},
   "source": [
    "# RMSprop優化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78947b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 初始值\n",
    "x = tf.Variable(-8.00000)\n",
    "y = tf.Variable(5.00000)\n",
    "\n",
    "# 定義目標函數\n",
    "def ObjFun():   \n",
    "    output = (0.5)*(x**2)+2.5*(y**2)\n",
    "    return output\n",
    "\n",
    "# 繪圖的函數，與目標函數相同\n",
    "def Draw_fun(x,y):    \n",
    "    z = (0.5)*(x**2)+2.5*(y**2)\n",
    "    return z\n",
    "\n",
    "# rho：衰減因子，也就是梯度方均根的衰減率\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.3,rho=0.9)\n",
    "\n",
    "epochs=50        # 疊代次數\n",
    "xValueArr=[-8]   # x的初始值\n",
    "yValueArr=[5]    # y的初始值\n",
    "for epoch in range(epochs):\n",
    "    opt.minimize(ObjFun, var_list=[x,y])\n",
    "    xValueArr.append(x.numpy())\n",
    "    yValueArr.append(y.numpy())\n",
    "\n",
    "x = np.arange(-10.0, 10.0, 0.01)\n",
    "y = np.arange(-10.0, 10.0, 0.01)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = Draw_fun(X,Y)\n",
    "plt.figure(figsize = (10,5))\n",
    "CS = plt.contour(X,Y,Z, colors = 'gray')\n",
    "plt.title(\"RMSprop Optimizer')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.plot(xValueArr, yValueArr, c='r')\n",
    "for xt, yt in zip(xValueArr,yValueArr):\n",
    "          plt.scatter(xt, yt, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928fceda",
   "metadata": {},
   "source": [
    "# Adam 優化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87826afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 初始值\n",
    "x = tf.Variable(-8.00000)\n",
    "y = tf.Variable(5.00000)\n",
    "\n",
    "# 定義目標函數\n",
    "def ObjFun():\n",
    "    output = (0.5)*(x**2)+2.5*(y**2)\n",
    "    return output\n",
    "\n",
    "# 要繪圖的函數，與目標函數相同\n",
    "def Draw_fun(x,y):    \n",
    "    z = (0.5)*(x**2)+2.5*(y**2)\n",
    "    return z\n",
    "\n",
    "# 選用一優化函式，作為迭代方法\n",
    "# beta_1：第一動量的指數衰減率，beta_2：第二動量的指數衰減率\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.3, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "epochs=50        # 疊代次數\n",
    "xValueArr=[-8]   # x的初始值\n",
    "yValueArr=[5]    # y的初始值\n",
    "for epoch in range(epochs):\n",
    "    opt.minimize(ObjFun, var_list=[x,y]) # 帶入函數，xy之最小值為何\n",
    "    xValueArr.append(x.numpy())\n",
    "    yValueArr.append(y.numpy())\n",
    "\n",
    "x = np.arange(-10.0, 10.0, 0.01)\n",
    "y = np.arange(-10.0, 10.0, 0.01)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = Draw_fun(X,Y)\n",
    "plt.figure(figsize = (10,5))\n",
    "CS = plt.contour(X,Y,Z, colors = 'gray')\n",
    "plt.title(\"Adam Optimizer')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.plot(xValueArr, yValueArr, c='r')\n",
    "for xt, yt in zip(xValueArr,yValueArr):\n",
    "          plt.scatter(xt, yt, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134a52dd",
   "metadata": {},
   "source": [
    "# 神經網路訓練實例(MNIST 手寫數字辨識)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15629192",
   "metadata": {},
   "source": [
    "訓練步驟：下載MNIST資料：這邊利用TF.Keras dataset抓取MNIST手寫辨識資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795e7808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "# 分為四部分：訓練集、測試集\n",
    "(train_Data, train_Label),(test_Data, test_Label) = mnist.load_data()\n",
    "# 查看mnist資料集大小\n",
    "print('train_data =',len(train_Data))  # 基本上有6萬筆資料\n",
    "print('test_data =',len(test_Data))\n",
    "# 查看mnist資料集維度\n",
    "print('train_data_dim =', train_Data.shape)\n",
    "print('test_data_dim =', test_Data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151c4380",
   "metadata": {},
   "source": [
    "這邊可以使用matplotlib輸出images數字影像，代碼如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931245d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_image(data):\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(4,4)\n",
    "    plt.imshow(data, cmap = 'binary')\n",
    "    plt.show()\n",
    "\n",
    "plot_image(train_Data[0])   # 每一張圖會對應到一個lable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bc91fd",
   "metadata": {},
   "source": [
    "這裡也可以印出訓練集第一筆的資料標籤值來驗證。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904c94b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train_Label[0] =', train_label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235e796b",
   "metadata": {},
   "source": [
    "設置超參數與資料訓練前處理：接下來要設定訓練網路所需要的超參數跟資料大小轉換，代碼如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e53b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 二維圖轉換成一維\n",
    "learning_rate = 0.01     # 學習率\n",
    "training_epoch = 1000    # 訓練次數\n",
    "batch_size = 2000        # 每次訓練大小\n",
    "\n",
    "# mnist資料的前置處理\n",
    "# 把訓練資料、測試資料拉成一維\n",
    "# 將原本是28X28的影像大小攤平成784，未來要當作輸入特徵\n",
    "train_Data_R, test_Data_R = train_Data.reshape([-1,784]).astype('float32')\\\n",
    "                           ,test_Data.reshape([-1,784]).astype('float32')\n",
    "# 訓練技巧：把資料正規化(把所有資料除以最大值)，可加快訓練速度\n",
    "train_Data_R, test_Data_R = train_Data_R/255., test_Data_R/255.  # 因每個點為8bit，0~255\n",
    "\n",
    "# 將資料打散並分批\n",
    "train_Data_M = tf.data.Dataset.from_tensor_slices((train_Data_R,train_Label))\n",
    "train_Data_M = train_Data_M.shuffle(5000).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0bcafa",
   "metadata": {},
   "source": [
    "設計網路"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7dfa98",
   "metadata": {},
   "source": [
    "這裡我們利用四層全連接層網路來當作是我們MNIST手寫文字辨識網路架構，輸入層有784筆資料，第一層隱藏層有256個節點，第二層隱藏層有128個節點，第三層隱藏層有64個節點，最後的輸出有十個節點，分別代表十個數字的機率大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e687cfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "\n",
    "(train_Data, train_Label),(test_Data, test_Label) = mnist.load_data()\n",
    "\n",
    "# 最後的Dense(10) 且activation用softmax\n",
    "# 代表最後output為10個class(0~9)的機率\n",
    "model = Sequential([\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a29c1cb",
   "metadata": {},
   "source": [
    "選擇優化器、損失函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cccfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 隨機梯度下降優化器\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "\n",
    "# 定義損失計算 交叉熵\n",
    "def cross_entropy_loss(x, y):  # x 預測值、y 真實值\n",
    "    # 選擇交叉商當損失函數\n",
    "    scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    # 計算損失\n",
    "    loss = scce(y, x) \n",
    "    # 計算平均損失\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018b3016",
   "metadata": {},
   "source": [
    "定義正確率函數，判斷測試後的正確程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feb80be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練圖的張數->維度0，\n",
    "# 預測->維度1(有10個值=數字0~9的機率值)，\n",
    "# 計算準確率\n",
    "def accuracy(y_pred, y_true):\n",
    "    # tf.argmax(y_pred, 1) 返回y_pred維度為1的最大索引跟正確值做比較 \n",
    "    # 求維度1最大值(機率值最高)的Index\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "    # 計算平均正確率\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b1ca9e",
   "metadata": {},
   "source": [
    "定義訓練與測試方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7e1af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 進行訓練\n",
    "for epoch in range(training_epoch):\n",
    "    for step, (batch_data, batch_label) in enumerate(train_Data_M): # 抽出資料，並標示批數、label\n",
    "        with tf.GradientTape() as tape:                       # 計算梯度\n",
    "            pre_data = model(batch_data)                      # 將數據丟進model內得到預測數值\n",
    "            loss = cross_entropy_loss(pre_data, batch_label)  # 將預測值丟入，計算損失\n",
    "            acc = accuracy(pre_data, batch_label)             # 計算正確率\n",
    "            trainable_variables = model.trainable_variables   # 將model內全部抽出\n",
    "            gradients = tape.gradient(loss, trainable_variables)  # 計算梯度->對損失函數微分\n",
    "        optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    # 每訓練完一個epoch，就拿測試集來測試準確率\n",
    "    Testprec = model(test_Data_R)\n",
    "    Testloss = cross_entropy_loss(Testprec, test_Label)\n",
    "    Testacc = accuracy(Testprec, test_Label)\n",
    "    print(\"Testloss: %f, Testaccuracy: %f\" % (Testloss, Testacc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
